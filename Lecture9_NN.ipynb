{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ab-sa/Statistical-Machine-Learning-2/blob/main/Lecture9_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fsvLxzhsubR4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, GridSearchCV\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivuMO5fPu1RO"
      },
      "outputs": [],
      "source": [
        "Credit = pd.read_csv('Credit.csv')\n",
        "Credit['Balance_multi'] = pd.cut(Credit.Balance, bins=[-1,300,700,2000],labels=[0,1,2])\n",
        "Credit = Credit.drop(['ID', 'Balance', 'Limit', 'Rating'], axis=1)\n",
        "print('Dimension of the data: ' + str(Credit.shape))\n",
        "print(Credit.head())\n",
        "print(Credit['Balance_multi'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5faj5Av7u8kM"
      },
      "source": [
        "**Artificial Neural Nets (ANN)**: Fit a classification NN with Balance_multi as response:\n",
        "- There are many different libraries with many different ways to a fit a NN in python. I use ***keras*** here, but feel free to look around for other options as well.\n",
        "- Let's start with fiting a simple ANN model with only two layers:\n",
        "  - layer 1: 10 hidden nodes\n",
        "  - layer 2: 6 hidden nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idqZRSFou9iI"
      },
      "outputs": [],
      "source": [
        "# data: dropping the credit-card related features (Limit & Rating) to make the classification problem more challenging\n",
        "X = pd.get_dummies(Credit.drop(['Balance_multi'], axis=1))\n",
        "y = Credit['Balance_multi']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# standardizing the input\n",
        "std_scale = StandardScaler().fit(x_train)\n",
        "X_train_std = std_scale.transform(x_train)\n",
        "X_test_std = std_scale.transform(x_test)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "encoded_y_train = encoder.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y_train = np_utils.to_categorical(encoded_y_train)\n",
        "\n",
        "# building the ANN model\n",
        "ANN_classifier = Sequential()\n",
        "# Defining the Input layer and FIRST hidden layer,both are same!\n",
        "# relu means Rectifier linear unit function\n",
        "# input_dim: number of features in your training data\n",
        "ANN_classifier.add(Dense(units=10, input_dim=13, activation='relu'))\n",
        "\n",
        "# Defining the SECOND hidden layer, here we have not defined input because it is\n",
        "# second layer and it will get input as the output of first hidden layer\n",
        "ANN_classifier.add(Dense(units=6, activation='relu'))\n",
        "\n",
        "# Defining the Output layer\n",
        "# for a classification problem, the output layer is defined as:\n",
        "# for a binary outcome: activation = 'softmax'\n",
        "# for a Multiclass (more than two classes) outcome: activation ='softmax'\n",
        "# And output_dim will be equal to the number of factor levels (here, it will be 3)\n",
        "ANN_classifier.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "# Optimizer: the algorithm of SGD to keep updating weights\n",
        "# loss: the loss function to measure the accuracy\n",
        "# metrics: the way we will compare the accuracy after each step of SGD\n",
        "ANN_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# fitting the Neural Network on the training data\n",
        "ANN_Model_1 = ANN_classifier.fit(X_train_std, dummy_y_train, batch_size=10 , epochs=10, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7edyziqq6-0"
      },
      "source": [
        "Measure the accuracy of our simple ANN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbKxT_exrF16"
      },
      "outputs": [],
      "source": [
        "# Predictions (scores) on testing data\n",
        "ANN_preds = ANN_classifier.predict(X_test_std)\n",
        "# Predictions (labels) on testing data\n",
        "ANN_preds_class = ANN_preds.argmax(axis=-1)\n",
        "\n",
        "print(classification_report(y_test, ANN_preds_class))\n",
        "print(confusion_matrix(y_test, ANN_preds_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning an ANN model by using GridSearchCV"
      ],
      "metadata": {
        "id": "BPC8oKh5AA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate Deep ANN model \n",
        "def make_classification_ann(Optimizer_Trial, Neurons_Trial_1, Neurons_Trial_2):\n",
        "    \n",
        "    # Creating the classifier ANN model\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units=Neurons_Trial_1, input_dim=13, activation='relu'))\n",
        "    classifier.add(Dense(units=Neurons_Trial_2, activation='relu'))\n",
        "    classifier.add(Dense(units=3, activation='softmax'))\n",
        "    classifier.compile(optimizer=Optimizer_Trial, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            \n",
        "    return classifier\n",
        "\n",
        "\n",
        "Parameter_Trials = {'batch_size':[10,20,30],\n",
        "                    'epochs':[10,20],\n",
        "#                    'Optimizer_Trial':['adam'],\n",
        "                    'Optimizer_Trial':['adam', 'rmsprop'],\n",
        "                    'Neurons_Trial_1': [5,10,30],\n",
        "                    'Neurons_Trial_2': [5,10,30]\n",
        "                   }\n",
        "\n",
        "# Creating the classifier ANN\n",
        "classifierModel = KerasClassifier(make_classification_ann, verbose=0)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "encoded_y = encoder.transform(y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_y)\n",
        "\n",
        "# Creating the Grid search space\n",
        "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
        "grid_search = GridSearchCV(estimator=classifierModel, param_grid=Parameter_Trials, cv=5)\n",
        "\n",
        "########################################\n",
        "\n",
        "# Measuring how much time it took to find the best params\n",
        "import time\n",
        "StartTime=time.time()\n",
        "\n",
        "# Running Grid Search for different paramenters\n",
        "grid_search.fit(X, dummy_y, verbose=0)\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
        "\n",
        "########################################\n",
        "\n",
        "# printing the best parameters\n",
        "print('#### Best hyperparamters ####')\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "QmFFhIlOANKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artificial Neural Nets (ANN)**: Fit a regression NN with Balance as response:\n"
      ],
      "metadata": {
        "id": "Om5fcIkwJmcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate Deep ANN model \n",
        "def make_regression_ann(Optimizer_Trial, Neurons_Trial_1, Neurons_Trial_2):\n",
        "    \n",
        "    # Creating the classifier ANN model\n",
        "    regressor = Sequential()\n",
        "    regressor.add(Dense(units=Neurons_Trial_1, input_dim=13, activation='relu'))\n",
        "    regressor.add(Dense(units=Neurons_Trial_2, activation='relu'))\n",
        "    regressor.add(Dense(units=1, activation='linear'))\n",
        "    regressor.compile(optimizer=Optimizer_Trial, loss='mean_squared_error', metrics=['mse'])\n",
        "            \n",
        "    return regressor\n",
        "\n",
        "\n",
        "Parameter_Trials = {'batch_size':[10,20,30],\n",
        "                    'epochs':[10,20],\n",
        "                    'Optimizer_Trial':['adam', 'rmsprop'],\n",
        "                    'Neurons_Trial_1': [5,10,30],\n",
        "                    'Neurons_Trial_2': [5,10,30]\n",
        "                   }\n",
        "\n",
        "# Creating the classifier ANN\n",
        "RegressionModel = KerasRegressor(make_regression_ann, verbose=0)\n",
        "\n",
        "# Creating the Grid search space\n",
        "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
        "grid_search_reg = GridSearchCV(estimator=RegressionModel, param_grid=Parameter_Trials, cv=5)\n",
        "\n",
        "########################################\n",
        "\n",
        "# Measuring how much time it took to find the best params\n",
        "import time\n",
        "StartTime=time.time()\n",
        "\n",
        "# Running Grid Search for different paramenters\n",
        "grid_search_reg.fit(X, y, verbose=0)\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
        "\n",
        "########################################\n",
        "\n",
        "# printing the best parameters\n",
        "print('#### Best hyperparamters ####')\n",
        "grid_search_reg.best_params_"
      ],
      "metadata": {
        "id": "YEogkJzfJpwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adjust the decay parameter (shrinkage) with keras, you need to modify the optimizer:"
      ],
      "metadata": {
        "id": "TyPXKpqFNhk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_regression_ann(decay_rate, Neurons_Trial_1, Neurons_Trial_2):\n",
        "\n",
        "    # you can also pass different values for momentum & lr (other inputs of SGD) to find their optimal values as well\n",
        "    sgd = SGD(decay=decay_rate, nesterov=False)\n",
        "\n",
        "    # Creating the classifier ANN model\n",
        "    regressor = Sequential()\n",
        "    regressor.add(Dense(units=Neurons_Trial_1, input_dim=13, activation='relu'))\n",
        "    regressor.add(Dense(units=Neurons_Trial_2, activation='relu'))\n",
        "    regressor.add(Dense(units=1, activation='linear'))\n",
        "    regressor.compile(optimizer=sgd, loss='mean_squared_error', metrics=['mse'])\n",
        "            \n",
        "    return regressor\n",
        "\n",
        "# for simplicity, only including the best parameters from the previouse GridSearchCV.\n",
        "# Idealy, you need to repeate the fitting process with all combinations again\n",
        "Parameter_Trials = {'batch_size':[10],\n",
        "                    'epochs':[20],\n",
        "                    'decay_rate':[1,0.01,0.001],\n",
        "                    'Neurons_Trial_1': [10],\n",
        "                    'Neurons_Trial_2': [30]\n",
        "                   }\n",
        "\n",
        "# Creating the classifier ANN\n",
        "RegressionModel = KerasRegressor(make_regression_ann, verbose=0)\n",
        "\n",
        "# Creating the Grid search space\n",
        "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
        "grid_search_reg = GridSearchCV(estimator=RegressionModel, param_grid=Parameter_Trials, cv=5)\n",
        "\n",
        "########################################\n",
        "\n",
        "# Measuring how much time it took to find the best params\n",
        "import time\n",
        "StartTime=time.time()\n",
        "\n",
        "# Running Grid Search for different paramenters\n",
        "grid_search_reg.fit(X, y, verbose=0)\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
        "\n",
        "########################################\n",
        "\n",
        "# printing the best parameters\n",
        "print('#### Best hyperparamters ####')\n",
        "grid_search_reg.best_params_"
      ],
      "metadata": {
        "id": "J4rnhRWXOEx0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lecture13_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoHjP8uBEyWCYpKsTQJa9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}